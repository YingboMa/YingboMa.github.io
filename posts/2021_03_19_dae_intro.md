+++
title = "A Survey of Differential-Algebraic Equations"
hascode = true
date = Date(2021, 3, 19)
rss = "We will introduce characterizations, numerical treatments, and structural
analysis/transformation algorithms of differential-algebraic equations in this
post."
+++

@def tags = ["math", "differential equations"]

# A Survey of Differential-Algebraic Equations

## Motivation

We know that the general form of an ODE system is $u'(t) = f(u(t), p, t)$ where
- $f$ is called the derivative function or the right hand side of the ODE
- $u$ is called the dependent variable or the states.
- $p$ is the parameters.
- $t$ is the independent variable.

In a freshman differential equations course you will learn [a bag of tricks]
(https://tutorial.math.lamar.edu/classes/de/de.aspx) to rearrange such equations
to analytically solve them, i.e. to find the formula for $u(t)$. However, most
of the ODEs are impossible to solve analytically, so in practice, we solve ODEs
numerically. This means that we need to come up with some iterative procedure
that approximates the function $u(t)$ for $t\in [a, b]\subseteq\R$ starting from
$u(a)$.

The basic idea behind all numerical ODE solvers is to arrange terms in a
particular way to truncate the Taylor series. For instance, let $h$ be the step
size. The famous explicit Euler method is
$$
u(t + h) = u(t) + h u'(t) + O(h^2) = u(t) + h f(t) + O(h^2).
$$
Of course, there's a wide array of methods to solve ODEs, but they all originate
from the idea of truncating Taylor series. For a more thorough introduction to
numerical ODEs, we direct it to ["Numerical solution of ordinary differential equations"](https://na.uni-tuebingen.de/~lubich/pcam-ode.pdf)
by Ernst Hairer and Christian Lubich.

In engineering, the right hand side of the ODE often don't have an explicit
formula. For instance, in a circuit, we know that the change of voltage is the
current over the capacitance, i.e. $\dd{V}{t} = I(t) / C$. Also, voltage and
current follow the Kirchhoff laws, which form a system of linear equations.
Hence, the governing equations of circuits naturally have both differential
equations and algebraic equations. [RC example. People don't realize it]

We also want to compose different components together to build a larger model,
and connection equations are algebraic relations among the state variables. It's
possible to reduce algebraic equations away so that we are only left with
explicit differential equations. However, this process is extremely time
consuming and error-prone. Also, we lost the composability and the
acausalness in the process. Hence, we want to use algorithms to automate the
simplification and simulation of differential-algebraic equations (DAEs) to
make modeling large scale simulation possible.

# Numerical methods for DAEs
Like we hinted before, ODEs with algebraic relations among its states are DAEs.
We can rephrase this as "differentiated states $u'(t)\in\R^n$ and state $u(t)
\in\R^n$ form an implicit relation together with parameters $p\in\R^m$ and the
independent variable $t\in\R$." Translating that into a formula, we have

$$ 0 = F(u'(t), u(t), p, t), $$

where $F: (\R^n, \R^n, \R^m, \R) \mapsto \R^n$.

Similar to ODEs, we can truncate the Taylor series of $u(t)$ to solve the above
system. The explicit Euler method becomes

$$ 0 = F\left(\frac{\left(u(t+h) + O(h^2)\right) - u(t)}{h},\; u(t),\; p,\; t+h\right). $$

Let's introduce $\hat{u}(t+h) \equiv u(t+h) + O(h^2)$ to denote the numerical
solution to avoid visual clutter. As we already need to solve the implicit
equations to approximate $u(t+h)$ using $\hat{u}(t+h)$, we can use the implicit
Euler method to gain more stability [This can be generalized to all the other
methods]

\begin{equation} \label{eq:dae_euler_eq}
0 = F\left(\frac{\hat{u}(t+h) - u(t)}{h},\; \hat{u}(t+h),\; p,\; t+h\right).
\end{equation}

Numerically, we use [Newton's method](https://en.wikipedia.org/wiki/Newton%27s_method#Nonlinear_systems_of_equations)
to solve potentially nonlinear equations by solving the best approximating
linear equations (i.e. the Jacobian of the nonlinear function with respect to
the unknowns) iteratively to refine the solution. By the chain rule, we have
\begin{equation}
\dd{F}{u} = \frac{1}{h}F_{u'} + F_{u}.
\end{equation}
The Newton iteration is then
\begin{align}
\left(\frac{1}{h}F_{u'} + F_{u}\right) \Delta^{[i]} &= F\left(\frac{\hat{u}^{[i]}(t+h) - u(t)}{h},\; \hat{u}^{[i]}(t+h),\; p,\; t+h\right) \\
\hat{u}^{[i+1]} &= \hat{u}^{[i]} - \Delta^{[i]},
\end{align}
where $\{\cdot\}^{[i]}$ denotes the iteration variable at the $i$-th iteration.
Astute reader will notice that the Jacobian in Newton's method is a matrix
pencil of the form $J(\lambda) = A + \lambda B$. We know that Newton's method
only makes sense when there exists a $\lambda \in \R$ (a suitable step size)
such that $\det(J(\lambda)) \ne 0$ (J is invertible), or in other words, the
matrix pencil is regular. We direct the reads to [Hairer II](https://www.springer.com/gp/book/9783540604525)
page 452 for more details regarding matrix pencils.

# Numerically motivated characterizations of DAEs

A natural question is: how can we relate DAEs to ODEs. Notice that $F$ is 0 for
all $t$, so the derivative of $F$ with respect to time also generates a new set
of equations, so do any higher order time derivatives of $F$, i.e.
\begin{align}
\dd{F}{t} &= 0 \\
\ddn{F}{t}{2} &= 0 \\
\ddn{F}{t}{3} &= 0 \\
    &\vdots
\end{align}
We need a way of knowing when differentiation $F$ won't add new "information"
into the system. First, let's develop a characterization on the variables. To do
that, let's introduce the $𝔇_S(x)$ function that gives us the order of the
highest derivative of the variable $x$ in a given system $S$. For instance, if
$x'''$ is the term with the highest derivative of $x$ in a system, then we have
$\frak{D} _S(x) = 3$. We can define $\eta = \{\frak{D}_S(x) > 0 : x \in u\}$ to
be the set of differential variables.  Similarly, the set of algebraic variables
is $\lambda = \{\frak{D}_S(x) = 0 : x \in u\}$. Note that $\eta$ and $\lambda$ must
be disjoint. Therefore, we can group the highest derivative terms
together as $z = (\eta';\; \lambda)$, and DAEs can then be written as
$$
0 = \hat{F}(z, \eta, p, t) \\
$$
Differentiating the above equation gives us
$$
\hat{F}_z z' + \hat{F}_y \eta' + \hat{F}_t = 0
$$
Note that $z'$ contains all the new terms generated by the differentiation, as
it contains variables with even higher order derivatives. Rearranging terms, we
get
$$
\hat{F}_z z' = -\hat{F}_y \eta' - \hat{F}_t.
$$
When $\hat{F}_z$ is invertible, we can use old terms to explicitly solve for
$z'$, so we don't generate genuine new terms. Therefore, we only add new
equations to the system if and only if $\hat{F}_z$ is fully ranked. It's very
wasteful to differentiate the entire system until the matrix is invertible. We
can only differentiate a minimal subset of equations to make $\hat{F}_z$ fully
ranked.

Also, note that the above equation is an ODE if $\hat{F}_z$ is invertible. We
can characterize DAEs by the number of differentiations needed to convert it
into an ODE, and we call this property the *index* of a DAE.
Now, let's translate the theory into a concrete example. The pendulum system
($P$) in Cartesian coordinate $(x, y)$ has the equations [a picture with labels]

\begin{align}
0 &= T(t) x(t) - x''(t)    \\
0 &= T(t) y(t) - g - y''(t)\\
0 &= x(t)^2 + y(t)^2 - L^2.
\end{align}

where $T$ is the tension from the string, $g$ is the gravitational acceleration,
$L$ is the length of the string. The highest order derivative terms are $z = (x'',
y'', T)$.

The exact rank of $\hat{F}_z\in \R^{n\times n}$ is too difficult to compute
because it's time-varying. In practice, we use the sparsity pattern or the
*structure* of the Jacobian to determine its structural rank — a weak version of
the numerical rank. We can use a bipartite graph to represent the sparsity
pattern. We define the structural rank as the maximum cardinality of the
bipartite matching. If we assume that no cancellation among the non-zero entries
are possible, then the structural rank is exactly the numerical rank. For more
details about the index-reduction algorithm, we suggest the [original paper by
Pantelides](https://epubs.siam.org/doi/10.1137/0909014).

The structure of $\hat{F}_z$ from the pendulum system is
\begin{align}
\begin{pmatrix}
\times & 0 & \times\\
0 & \times & \times\\
0 & 0 & 0\\
\end{pmatrix},
\end{align}
which is clearly singular. Since only the last row of the matrix is problematic,
we only need to use differentiations to introduce $x'', y''$, or $T$ into the
last equation to make the Jacobian structurally fully ranked. Differentiating
the last equation twice, we get
\begin{align}
2xx' + 2yy' &= 0 \\
2 x'^2+2 x x''+2 y'^2+2 y y'' &= 0.
\end{align}

Now, the structure of $\hat{F}_z$ is
\begin{align}
\begin{pmatrix}
\times & 0 & \times\\
0 & \times & \times\\
\times & \times & 0
\end{pmatrix},
\end{align}
which has full structural rank. Now, we can use Newton's method to solve for the
unknowns in the differentiated system
\begin{align}
T(t) x(t) - x''(t)       &= 0\\
T(t) y(t) - g - y''(t)   &= 0\\
2 x'^2+2 x x''+2 y'^2+2 y y'' &= 0.
\end{align}
We call DAE systems with fully ranked Jacobian like this an index 1 DAE. The
original pendulum system is index 3, since we differentiated the last equation
twice. As a base case, we call the system index 0 DAE when the minimum of
$\frak{D}$ is 1, while the Jacobian is fully ranked.
